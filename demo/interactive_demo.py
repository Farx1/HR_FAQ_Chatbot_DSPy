"""
Interactive demo for HR FAQ chatbot
Provides a simple command-line interface to test the fine-tuned model
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import warnings
warnings.filterwarnings("ignore")

# Model configuration
MODEL_NAME = "mistralai/Mistral-7B-Instruct-v0.3"

def load_model():
    """Load the trained model and tokenizer"""
    
    print("Loading HR FAQ chatbot...")
    
    try:
        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained("models/hr_faq_mistral_lora")
        
        # Load base model with quantization
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16
        )
        
        base_model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            quantization_config=bnb_config,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True
        )
        
        # Load LoRA adapters
        model = PeftModel.from_pretrained(base_model, "models/hr_faq_mistral_lora_adapters")
        
        print("‚úì Model loaded successfully!")
        return model, tokenizer
        
    except Exception as e:
        print(f"‚úó Error loading model: {e}")
        print("Please make sure you have trained the model first by running:")
        print("  python training/train.py")
        return None, None

def generate_response(model, tokenizer, question: str) -> str:
    """Generate response for a given question"""
    
    system_prompt = "Tu es un assistant RH professionnel. R√©ponds de fa√ßon claire, concise et exacte sur la base des politiques RH disponibles. Si la question sort du p√©rim√®tre RH ou si l'information manque, indique-le poliment et propose de contacter le service RH."
    
    # Format prompt
    prompt = f"<s>[INST] {system_prompt}\n\n{question} [/INST]"
    
    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.2,
            top_p=0.9,
            repetition_penalty=1.05,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Decode response
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Extract only the generated part
    response = response.split("[/INST]")[-1].strip()
    
    return response

def print_welcome():
    """Print welcome message"""
    
    print("=" * 60)
    print("ü§ñ CHATBOT FAQ RH - MISTRAL AI")
    print("=" * 60)
    print("Assistant RH professionnel fine-tun√© sur Mistral-7B-Instruct-v0.3")
    print("")
    print("üí° Exemples de questions:")
    print("  ‚Ä¢ Combien de jours de cong√© ai-je par an ?")
    print("  ‚Ä¢ Quelle est la politique de t√©l√©travail ?")
    print("  ‚Ä¢ Comment signaler un harc√®lement ?")
    print("  ‚Ä¢ Quelles formations sont disponibles ?")
    print("")
    print("‚ùå Questions hors domaine:")
    print("  ‚Ä¢ Comment installer Python ?")
    print("  ‚Ä¢ Quelle est la capitale de la France ?")
    print("")
    print("Tapez 'quit' pour quitter, 'help' pour l'aide")
    print("=" * 60)

def print_help():
    """Print help message"""
    
    print("\nüìã AIDE - CHATBOT FAQ RH")
    print("-" * 30)
    print("Ce chatbot r√©pond aux questions sur les politiques RH de l'entreprise.")
    print("")
    print("üéØ Domaines couverts:")
    print("  ‚Ä¢ Cong√©s et absences")
    print("  ‚Ä¢ Contrats et salaires")
    print("  ‚Ä¢ T√©l√©travail")
    print("  ‚Ä¢ Formation et d√©veloppement")
    print("  ‚Ä¢ Recrutement")
    print("  ‚Ä¢ Harc√®lement et s√©curit√©")
    print("")
    print("‚ö†Ô∏è  Le chatbot refusera poliment les questions hors domaine RH.")
    print("")
    print("üîß Commandes:")
    print("  ‚Ä¢ 'quit' ou 'exit' : Quitter")
    print("  ‚Ä¢ 'help' : Afficher cette aide")
    print("  ‚Ä¢ 'clear' : Effacer l'√©cran")
    print("")

def run_demo():
    """Main demo function"""
    
    # Print welcome
    print_welcome()
    
    # Load model
    model, tokenizer = load_model()
    
    if model is None or tokenizer is None:
        return
    
    print("\nüöÄ Pr√™t √† r√©pondre √† vos questions RH !")
    print("")
    
    # Main interaction loop
    while True:
        try:
            # Get user input
            question = input("‚ùì Votre question RH: ").strip()
            
            # Handle special commands
            if question.lower() in ['quit', 'exit', 'q']:
                print("\nüëã Merci d'avoir utilis√© le chatbot RH. √Ä bient√¥t !")
                break
            
            elif question.lower() == 'help':
                print_help()
                continue
            
            elif question.lower() == 'clear':
                os.system('cls' if os.name == 'nt' else 'clear')
                print_welcome()
                continue
            
            elif not question:
                print("‚ö†Ô∏è  Veuillez poser une question.")
                continue
            
            # Generate response
            print("\nü§ñ R√©ponse:")
            print("-" * 40)
            
            response = generate_response(model, tokenizer, question)
            print(response)
            
            print("-" * 40)
            print("")
            
        except KeyboardInterrupt:
            print("\n\nüëã Interruption d√©tect√©e. Au revoir !")
            break
        
        except Exception as e:
            print(f"\n‚ùå Erreur: {e}")
            print("Veuillez r√©essayer.")

def run_batch_test():
    """Run batch test with predefined questions"""
    
    print("üß™ Test par lot - Questions pr√©d√©finies")
    print("=" * 50)
    
    # Load model
    model, tokenizer = load_model()
    
    if model is None or tokenizer is None:
        return
    
    # Test questions
    test_questions = [
        "How many vacation days do I get per year?",
        "What is the remote work policy?",
        "How do I report workplace harassment?",
        "What training opportunities are available?",
        "How do I request time off?",
        "What is the dress code policy?",
        "How do I install Python on my computer?",  # OOD
        "What is the capital of France?",  # OOD
        "How do I bake a chocolate cake?",  # OOD
    ]
    
    print(f"Test de {len(test_questions)} questions...")
    print("")
    
    for i, question in enumerate(test_questions, 1):
        print(f"Question {i}: {question}")
        print("R√©ponse:")
        
        response = generate_response(model, tokenizer, question)
        print(response)
        
        print("-" * 60)
        print("")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        run_batch_test()
    else:
        run_demo()
