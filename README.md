# HR FAQ Chatbot ‚Äî DSPy-Optimized

An intelligent HR assistant chatbot built with fine-tuned LLMs and DSPy optimization to answer employee questions about HR policies, benefits, payroll, and company procedures with high accuracy and out-of-domain detection.

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![Hugging Face](https://img.shields.io/badge/ü§ó_Transformers-FFD21E?style=for-the-badge)
![DSPy](https://img.shields.io/badge/DSPy-Optimized-00D4AA?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)
![CI](https://github.com/Farx1/HR_FAQ_Chatbot_DSPy/workflows/CI/badge.svg?style=for-the-badge)

</div>

---

## üöÄ Overview

This project implements a production-ready HR FAQ chatbot that combines:

- **Fine-tuned language models** (LoRA/QLoRA) for domain-specific knowledge
- **DSPy framework** for automatic prompt optimization, achieving +784% ROUGE-L improvement
- **RAG (Retrieval-Augmented Generation)** for context-aware responses from company documents
- **Out-of-domain detection** with 90% rejection rate for non-HR questions
- **Modern web interface** with real-time streaming responses

Built for **HR departments** and **companies** looking to automate employee support, with a focus on **reliability**, **accuracy**, and **statistical validation** of improvements.

---

## üß∞ Tech Stack

- **Languages**: Python 3.8+, TypeScript
- **Frameworks**: FastAPI (backend), Next.js 16 (frontend), React 19
- **AI / ML**: PyTorch, Transformers, PEFT (LoRA), DSPy, Sentence Transformers
- **Data / Storage**: ChromaDB (vector database), JSON (datasets), SQLite
- **Tools**: Uvicorn, Mantine UI, Framer Motion, Rouge/BLEU evaluation

---

## ‚ú® Features

- **Fine-tuned LLM** ‚Äì LoRA/QLoRA parameter-efficient fine-tuning on Mistral-7B or DialoGPT for domain adaptation
- **DSPy Integration** ‚Äì Automatic prompt optimization achieving +784% ROUGE-L improvement over baseline
- **OOD Detection** ‚Äì 90% rejection rate for off-topic questions with keyword-based classification
- **RAG Support** ‚Äì Retrieval-augmented generation from company HR documents (policies, benefits, payroll)
- **Professional Benchmarks** ‚Äì Statistical significance testing (p<0.001) with comprehensive error analysis
- **Modern Web UI** ‚Äì Next.js frontend with real-time streaming, responsive design, and premium UX
- **Production Ready** ‚Äì Complete pipeline from training to deployment with health checks and error handling

---

## üß† How it works (high-level)

- **Frontend**: Built with Next.js and React, handles the chat interface, real-time streaming via Server-Sent Events (SSE), and displays metrics/confidence scores.

- **Backend / API**: FastAPI server that:
  - Processes questions through OOD detection (early rejection of non-HR queries)
  - Retrieves relevant context from company documents using ChromaDB vector search
  - Generates answers using fine-tuned models with DSPy-optimized prompts
  - Streams responses back to frontend for real-time UX

- **Data**: 
  - Company HR documents stored in `company_data/` (policies, benefits, payroll)
  - Vector embeddings in ChromaDB for semantic search
  - Training data in Alpaca format for fine-tuning

- **AI / ML part**:
  - Uses fine-tuned DialoGPT-small or Mistral-7B with LoRA adapters for parameter efficiency
  - DSPy framework optimizes prompts automatically using ChainOfThought reasoning
  - RAG engine retrieves relevant document snippets based on semantic similarity
  - Keyword-based OOD detection filters out non-HR questions before processing

---

## üì¶ Project Structure

- `backend/` ‚Äì FastAPI server with RAG engine (`server.py`, `rag_engine.py`)
- `webapp/` ‚Äì Next.js frontend application (`src/app/page.tsx` for main UI)
- `dspy_module/` ‚Äì DSPy integration (`hr_faq_dspy.py` for adapter and modules)
- `training/` ‚Äì Fine-tuning scripts (`train.py` for GPU, `train_cpu.py` for CPU)
- `evaluation/` ‚Äì Evaluation and benchmark scripts
- `demo/` ‚Äì Interactive command-line demos
- `data/` ‚Äì Training datasets (Alpaca format) and test sets
- `models/` ‚Äì Fine-tuned LoRA adapters and checkpoints
- `company_data/` ‚Äì HR documents for RAG (policies, benefits, payroll)
- `reports/` ‚Äì Benchmark results and analysis (`reports/latest_run/` for structured outputs)

---

## üß™ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/Farx1/HR_FAQ_Chatbot_DSPy.git
cd HR_FAQ_Chatbot_DSPy
```

### 2. Install dependencies

**Python dependencies:**
```bash
pip install -r requirements.txt
pip install -r backend/requirements.txt
```

**Frontend dependencies:**
```bash
cd webapp
npm install
cd ..
```

### 3. Environment variables (optional)

**Backend:** Copy `backend/.env.example` to `backend/.env` if you need to customize configuration (optional, defaults work for most cases).

**Frontend:** Copy `webapp/.env.local.example` to `webapp/.env.local` if you need to customize the backend URL:

```bash
NEXT_PUBLIC_API_URL=http://localhost:8000
```

The default is `http://localhost:8000`, so this is only needed if your backend runs on a different port or host.

### 3.5. Verify fresh clone (optional but recommended)

To verify that everything is set up correctly, run the verification script:

**Linux/Mac:**
```bash
chmod +x scripts/verify_fresh_clone.sh
./scripts/verify_fresh_clone.sh
```

**Windows (PowerShell):**
```powershell
.\scripts\verify_fresh_clone.ps1
```

This script will:
- Create a Python virtual environment
- Install all dependencies
- Run linting (ruff)
- Run all tests (pytest)
- Test backend imports
- Test health endpoint (FastAPI TestClient)
- Build the frontend
- Verify everything works offline (no external API calls)

If all checks pass, your environment is ready for development!

### 4. Run the app

**Option 1: One-command startup (recommended)**

```bash
# If you have Make installed (Linux/Mac)
make dev

# Or use the scripts directly:
# Linux/Mac
./scripts/dev.sh
# or
./start.sh

# Windows
.\scripts\dev.ps1
# or
.\start.ps1
```

This will:
- Create a Python virtual environment if missing
- Install all dependencies (Python + Node.js)
- Start the backend on port 8000
- Start the frontend on port 3000
- Wait for backend health check
- Print URLs and success message

**Option 2: Manual startup**

Terminal 1 (Backend):
```bash
cd backend
uvicorn server:app --reload --port 8000
```

Terminal 2 (Frontend):
```bash
cd webapp
npm run dev
```

Then open: **http://localhost:3000**

### 5. Test the chatbot

- Ask HR questions like: "How many vacation days do I get per year?"
- Try OOD questions like: "What is the capital of France?" (should be rejected)
- Check the quality panel for confidence scores and OOD detection status

---

## üìä Results

### Performance Comparison

| Metric | Baseline | DSPy Optimized | Improvement |
|--------|:--------:|:--------------:|:-----------:|
| **ROUGE-L** | 0.014 | **0.126** | **+784.6%** |
| **OOD Rejection** | 0% | **90%** | **+90.0%** |
| **Latency** | 343ms | **272ms** | **-20.7%** |

**Statistical Significance**: All improvements are statistically significant (p<0.001) with large effect sizes (Cohen's d > 1.0).

### Benchmark Output

Run the professional benchmark:
```bash
# Activate virtual environment first (if using venv)
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows

# Run benchmark
python benchmark_professional.py
```

Results are saved to `reports/latest_run/`:
- `metrics.json` ‚Äì Summary statistics
- `predictions.jsonl` ‚Äì All individual predictions
- `config.yaml` ‚Äì Benchmark configuration
- `report.md` ‚Äì Human-readable report
- `professional_benchmark_results.json` ‚Äì Full results JSON

### Reproducing Results

**Baseline vs DSPy Optimized:**
- **Baseline**: Fine-tuned model without DSPy optimization (direct inference)
- **DSPy Optimized**: Same model with DSPy ChainOfThought prompt optimization

**To reproduce the +784% ROUGE-L improvement:**
1. Ensure you have the fine-tuned model in `models/hr_faq_dialogpt_lora/`
2. Run `python benchmark_professional.py`
3. Compare baseline vs DSPy results in `reports/latest_run/metrics.json`
4. The improvement is calculated as: `(DSPy_ROUGE_L - Baseline_ROUGE_L) / Baseline_ROUGE_L * 100`

**Note**: Results may vary slightly due to model initialization and random seeds. The benchmark uses a fixed seed (42) for reproducibility.

---

## üìö What I learned

- **DSPy Framework** ‚Äì Learned how to integrate DSPy for automatic prompt optimization, achieving significant improvements in response quality without manual prompt engineering
- **RAG Implementation** ‚Äì Built a retrieval-augmented generation system using ChromaDB and sentence transformers for context-aware responses from company documents
- **Parameter-Efficient Fine-tuning** ‚Äì Applied LoRA/QLoRA techniques to fine-tune large language models efficiently, reducing memory requirements while maintaining performance
- **Statistical Validation** ‚Äì Implemented proper statistical testing (paired t-tests, confidence intervals, effect sizes) to validate model improvements scientifically
- **Full-Stack AI Application** ‚Äì Integrated fine-tuned models, RAG, and DSPy into a production-ready web application with real-time streaming and error handling
- **Evaluation Metrics** ‚Äì Gained deep understanding of ROUGE-L, BLEU, and OOD detection metrics for chatbot evaluation

---

## üîÆ Possible improvements

- **Multi-language support** ‚Äì Extend to support questions in multiple languages (French, Spanish, etc.)
- **User authentication** ‚Äì Add login system to personalize responses based on employee role/department
- **Feedback loop** ‚Äì Implement user feedback collection to continuously improve the model
- **Advanced RAG** ‚Äì Experiment with re-ranking, multi-hop retrieval, and hybrid search (keyword + semantic)
- **Additional DSPy optimizers** ‚Äì Try MIPRO, GEPA, or other DSPy optimizers for further improvements
- **Deployment** ‚Äì Containerize with Docker and deploy to cloud (AWS, GCP, Azure) with auto-scaling
- **Monitoring & Logging** ‚Äì Add comprehensive logging, metrics collection (Prometheus), and alerting
- **Testing** ‚Äì Expand unit tests, integration tests, and end-to-end tests for reliability
- **Documentation** ‚Äì Add API documentation (OpenAPI/Swagger) and user guides

---

## üë§ About me (Jules)

I'm **Jules**, a M2 **Data & AI Engineering** student at **ESILV (Paris, France)**,  
with a focus on **LLMs, agentic AI, privacy-preserving ML, and quantum computing**.

- üéì Currently: M2 Data & IA, Quantum track  
- üíº Looking for: **6-month end-of-studies internship (Data / ML / LLM)** starting **February 2026**  
- üåê Portfolio: https://julesbarth-myportfolio.fr  
- üíº LinkedIn: https://www.linkedin.com/in/jules-barth  
- üìß Email: julesbarth13@gmail.com  

Feel free to reach out if this project resonates with what you're building. üöÄ

---

## üìù License

This project is licensed under the **MIT License** ‚Äî see [LICENSE](LICENSE) for details.

---

## üôè Acknowledgments

- **[Mistral AI](https://mistral.ai/)** ‚Äî Base model
- **[DSPy](https://github.com/stanfordnlp/dspy)** ‚Äî Optimization framework
- **[Hugging Face](https://huggingface.co/)** ‚Äî Transformers & PEFT
- **[Vercel](https://vercel.com/)** ‚Äî Next.js framework
